{"cells":[{"cell_type":"markdown","metadata":{"id":"--iZtJK0CtKs"},"source":["#Connect to google driver"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TbK6x3pP7b0P","outputId":"7b1d1414-a7c1-4654-e118-c7efb849b4b5","executionInfo":{"status":"ok","timestamp":1681497089608,"user_tz":-120,"elapsed":2088,"user":{"displayName":"ziyuan rao","userId":"11142835866797234336"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"mVa5s4qW8JAE"},"source":["connecte to google drive if you use google colab"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":271,"status":"ok","timestamp":1681497092825,"user":{"displayName":"ziyuan rao","userId":"11142835866797234336"},"user_tz":-120},"id":"OaOFOgLy8LAM","outputId":"594e16db-781a-4340-dff3-e73407bba028"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/APT_CNN\n"]}],"source":["%cd /content/drive/MyDrive/APT_CNN"]},{"cell_type":"markdown","metadata":{"id":"7wU5-F5w8NbT"},"source":["link to the folder which store your files"]},{"cell_type":"markdown","metadata":{"id":"Jni2elyt-D2H"},"source":["#Creat the sythetic data"]},{"cell_type":"markdown","metadata":{"id":"PS6_vOnr-0w_"},"source":["##1.Euler transformation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YjghTy3D-yxA"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import math\n","\n","def Euler_transformation_100(data_clean, plot):\n","    #%% Plot 3D crystal structure without duplicates\n","    data_110 = data_clean\n","    if plot == True:\n","        df = pd.DataFrame(data_clean,columns=['a','b','c','d'])\n","        group_1 = df[df.d==56].values\n","        group_2 = df[df.d==24].values\n","        fig = plt.figure()\n","        ax = plt.subplot(111, projection='3d')  # build a project\n","        ax.scatter(data_clean [:, 0], data_clean [:, 1], data_clean [:, 2], c=data_clean [:, 3], s=8)  # 绘制数据点\n","        #ax.plot(group_1 [:, 0], group_1 [:, 2], 'p', zdir = 'y', label = 'Al')    #divide it into different parts\n","        #ax.plot(group_2 [:, 0], group_2 [:, 2], 'o', zdir = 'y', label = 'Mg')    #divide it into different parts\n","        \n","        # plt.legend(loc='upper right')\n","        ax.tick_params(axis='both', which='major', labelsize=16)\n","        ax.set_zlabel('Z', fontsize=16)  # axis\n","        ax.set_ylabel('Y', fontsize=16)\n","        ax.set_xlabel('X', fontsize=16)\n","        plt.show()\n","    return data_110\n","#%%\n","def Euler_transformation_110(data_clean, plot):\n","    #%%  change pole from 100 to 110\n","    row = data_clean.shape[0]\n","    data_110 = np.empty([row,4], dtype = float)\n","    data_110[:,3] = data_clean[:,3]\n","    \n","    ##rotate along x axis\n","    #data_110[:,0] = data_sample[:,0]\n","    #data_110[:,1] = (data_sample[:,1]+data_sample[:,2])*math.sqrt(2)/2.0\n","    #data_110[:,2] = (data_sample[:,1]+data_sample[:,2])*math.sqrt(2)/2.0\n","    \n","    #rotate along y axis\n","    data_110[:,0] = (data_clean[:,0]-data_clean[:,2])*math.sqrt(2)/2.0\n","    data_110[:,1] = data_clean[:,1]\n","    data_110[:,2] = (data_clean[:,0]+data_clean[:,2])*math.sqrt(2)/2.0\n","    \n","    ##rotate along Z axis\n","    #data_110[:,0] = (data_sample[:,0]+data_sample[:,1])*math.sqrt(2)/2.0\n","    #data_110[:,1] = (-data_sample[:,0]+data_sample[:,1])*math.sqrt(2)/2.0\n","    #data_110[:,2] = data_sample[:,2]\n","    #%% Plot 3D crystal structure 011\n","    if plot == True:\n","        df = pd.DataFrame(data_110,columns=['a','b','c','d'])\n","        group_1 = df[df.d==56].values\n","        group_2 = df[df.d==24].values\n","        \n","        fig = plt.figure()\n","        ax = plt.subplot(111, projection='3d')  # build a project\n","        ax.scatter(data_110 [:, 0], data_110 [:, 1], data_110 [:, 2],c=data_110 [:, 3], s=8)  # 绘制数据点\n","        #ax.plot(group_1 [:, 0], group_1 [:, 2], 'p', zdir = 'y', label = 'Al')    #divide it into different parts\n","        #ax.plot(group_2 [:, 0], group_2 [:, 2], 'o', zdir = 'y', label = 'Mg')    #divide it into different parts\n","        \n","        # plt.legend(loc='upper right')\n","        ax.tick_params(axis='both', which='major', labelsize=16)\n","        ax.set_zlabel('Z', fontsize=16)  # axis\n","        ax.set_ylabel('Y', fontsize=16)\n","        ax.set_xlabel('X', fontsize=16)\n","        plt.show()\n","    #%%\n","    return data_110\n","#%%\n","def Euler_transformation_111(data_clean, plot):\n","    #%%  change pole from 111 to 001\n","    row = data_clean.shape[0]\n","    data_111 = np.empty([row,4], dtype = float)\n","    data_111[:,3] = data_clean[:,3]\n","    \n","    #rotate [111]\n","    data_111[:,0] = data_clean[:,0]*0.408248 + data_clean[:,1]*0.408248 + data_clean[:,2]*(-0.816497)\n","    data_111[:,1] = data_clean[:,0]*(-0.707107) + data_clean[:,1]*0.707107 + data_clean[:,2]*(0.0)\n","    data_111[:,2] = data_clean[:,0]*0.57735 + data_clean[:,1]*(0.57735) + data_clean[:,2]*(0.57735)\n","    \n","    data_110 = data_111\n","    #%% Plot 3D crystal structure 111\n","    if plot == True:  \n","        df = pd.DataFrame(data_110,columns=['a','b','c','d'])\n","        group_1 = df[df.d==56].values  #Al\n","        group_2 = df[df.d==7].values  #Li\n","        group_3 = df[df.d==24].values   #Mg\n","        \n","        fig = plt.figure()\n","        ax = plt.subplot(111, projection='3d')  # build a project\n","        \n","        ax.scatter(data_110 [:, 0], data_110 [:, 1], data_110 [:, 2],c=data_110 [:, 3], s=8)  # 绘制数据点\n","        \n","        ax.tick_params(axis='both', which='major', labelsize=16)\n","        ax.set_zlabel('Z', fontsize=16)  # axis\n","        ax.set_ylabel('Y', fontsize=16)\n","        ax.set_xlabel('X', fontsize=16)\n","        plt.show()\n","    #%%\n","    return data_110"]},{"cell_type":"markdown","metadata":{"id":"lBdO5Y6A_HL-"},"source":["##2. creat sythetic data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pixGQ8VVBKth"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import pandas as pd\n","from numpy import random\n","import os\n","\n","def add_data_noise(data_reconstruction, sigma_xy, sigma_z, plot_noise):\n","   #%% adding noise with the same dimension as 'data_clean'\n","    # print('---->>')\n","    # print('sigma_xy=',sigma_xy)\n","    # print('sigma_z=',sigma_z)\n","    row = data_reconstruction.shape[0]\n","    noise_xy = np.random.normal(mu, sigma_xy, [row ,2]) \n","    noise_z = np.random.normal(mu, sigma_z, [row ,1])\n","    zeros = np.zeros((row, 1))\n","    noise = np.hstack((noise_xy, noise_z, zeros))\n","    data_noise = data_reconstruction + noise\n","    # print(data_noise.type)\n","    # Plot 3D crystal structure with noise\n","    if plot_noise == True:\n","        ax = plt.subplot(111, projection='3d')  # build a project\n","        ax.scatter(data_noise [:, 0], data_noise [:, 1], data_noise [:, 2], c=data_noise [:, 3], s=8)  # 绘制数据点\n","        ax.tick_params(axis='both', which='major', labelsize=16)\n","        ax.set_zlabel('Z', fontsize=16)  # axis\n","        ax.set_ylabel('Y', fontsize=16)\n","        ax.set_xlabel('X', fontsize=16)\n","        plt.show()\n","    return data_noise\n","#%% Input file and parameters  fcc data\n","i=0\n","while i < 200: #we only creat 200 data here to save the demo time\n","    data_o = np.loadtxt('ggoutputFile_fcc_5nm_a_0.405.txt')\n","    lattice_para = 0.405\n","    mu  = 0\n","    sigma_xy_all = np.arange(0.2,0.8,0.2)\n","    sigma_z_all = np.arange(0.03,0.055,0.002)\n","    # replace some atoms if it is needed\n","    data = pd.DataFrame(data_o)\n","    rand = random.randint(8,13)\n","    replace_n = round(data.shape[0]*0.01*rand)\n","    replace_indices = np.random.choice(data.index, replace_n, replace = False)   \n","    data.iloc[replace_indices,3] = data.iloc[replace_indices,3].replace(27,24).to_numpy()\n","    # rand = random.randint(1,51)\n","    rand = 43\n","    remove_n = round(data.shape[0]*0.01*rand)\n","    drop_indices = np.random.choice(data.index, remove_n, replace = False)\n","    data_1=data.drop(drop_indices)\n","    data_1=data_1.reset_index()\n","    data_1=data_1.drop(columns=['index'])\n","    # detect_eff_array = np.arange(0.35,0.8,0.02)\n","    # atomic_number_1 = 27  #Al\n","    # atomic_number_2 = 24  #Mg\n","    plot_noise = False #default\n","    # image_name_1 = \"AlMgLi_L12_AlAl\"\n","    # image_name_2 = \"AlMgLi_L12_MgMg\"\n","    #%% Remove all duplicates    \n","    data_clean = np.unique(data_1, axis=0)\n","    #%% Euler_transformation\n","    # data_reconstruction = Euler_transformation_100(data_clean, False)\n","    data_reconstruction = Euler_transformation_110(data_clean, False)\n","    # data_reconstruction = Euler_transformation_111(data_clean, False)\n","#%% Generating data with noise\n","    folder_dir = 'simulated_data'\n","    if not os.path.isdir(folder_dir):\n","        os.mkdir(folder_dir)\n","    for sigma_xy in sigma_xy_all:\n","        for sigma_z in sigma_z_all:\n","            data_noise = add_data_noise(data_reconstruction, sigma_xy, sigma_z, plot_noise)\n","#%%images into H5f\n","            data_noise=pd.DataFrame(data_noise,columns=['x','y','z','Da'])\n","            data_noise.to_csv('simulated_data/data_fcc_{}.csv'.format(i))\n","            print(i)\n","            i+=1\n","i=0\n","while i < 200:#we only creat 200 data here to save the demo time\n","    data_o = np.loadtxt('ggoutputFile_L12_5nm_a_0.405.txt')\n","    lattice_para = 0.405\n","    mu  = 0\n","    sigma_xy_all = np.arange(0.2,0.8,0.2)\n","    sigma_z_all = np.arange(0.03,0.055,0.002)\n","    # replace some atoms if it is needed\n","    data = pd.DataFrame(data_o)\n","    # rand = 13,95\n","    # replace_n = round(data.shape[0]*0.01*rand)\n","    # replace_indices = np.random.choice(data.index, replace_n, replace = False)   \n","    # data.iloc[replace_indices,3] = data.iloc[replace_indices,3].replace(24,27).to_numpy()\n","    # rand = random.randint(1,51)\n","    rand = 43\n","    remove_n = round(data.shape[0]*0.01*rand)\n","    drop_indices = np.random.choice(data.index, remove_n, replace = False)\n","    data_1=data.drop(drop_indices)\n","    data_1=data_1.reset_index()\n","    data_1=data_1.drop(columns=['index'])\n","    plot_noise = False #default\n","    data_clean = np.unique(data_1, axis=0)\n","    data_reconstruction = Euler_transformation_110(data_clean, False)\n","    folder_dir = 'simulated_data'\n","    if not os.path.isdir(folder_dir):\n","        os.mkdir(folder_dir)\n","    for sigma_xy in sigma_xy_all:\n","        for sigma_z in sigma_z_all:\n","            data_noise = add_data_noise(data_reconstruction, sigma_xy, sigma_z, plot_noise)\n","            data_noise=pd.DataFrame(data_noise,columns=['x','y','z','Da'])\n","            data_noise.to_csv('simulated_data/data_l12_{}.csv'.format(i))\n","            print(i)\n","            i+=1\n","            "]},{"cell_type":"markdown","metadata":{"id":"PLpFywjzGiSK"},"source":["#Voxelization"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":553443,"status":"ok","timestamp":1681378356203,"user":{"displayName":"ziyuan rao","userId":"11142835866797234336"},"user_tz":-120},"id":"NHLniaeaGq05","outputId":"9744dfac-4f6d-4665-e2f8-d062d35bfe06"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 200/200 [09:12<00:00,  2.76s/it]\n","100%|██████████| 200/200 [00:00<00:00, 359409.08it/s]\n"]}],"source":["\n","import numpy as np\n","import pandas as pd\n","import re\n","from tqdm import tqdm\n","import h5py\n","from sklearn.model_selection import train_test_split\n","\n","def voxelize(points):\n","    #n indicates the voxel size\n","    n = 8\n","    padding_size=(n, n, n, 2)\n","    voxels = np.zeros(padding_size)\n","    points = points.values\n","    origin = (np.min(points[:, 1]), np.min(points[:, 2]), np.min(points[:, 3]))\n","    # set the nearest point as (0,0,0)\n","    points[:, 1] -= origin[0]\n","    points[:, 2] -= origin[1]\n","    points[:, 3] -= origin[2]\n","    \n","    points=pd.DataFrame(points)\n","    \n","    # Atom_Al = pd.DataFrame(columns=['number','x','y','z','Da'])\n","    Atom_Mg = points[points.iloc[:, 4] == 24 ]\n","    \n","    # Atom_Fe = pd.DataFrame(columns=['number','x','y','z','Da'])\n","    Atom_Al = points[points.iloc[:, 4] == 27 ]\n","    \n","    \n","    for i in range(n):\n","        for j in range(n):\n","            for k in range(n):\n","                min_x = i*(8/n) #here 8 is the size of the simulated data, i.e., 8 nm. n is the voxle size\n","                max_x = (i+1)*(8/n)\n","                min_y = j*(8/n)\n","                max_y = (j+1)*(8/n)\n","                min_z = k*(8/n)\n","                max_z = (k+1)*(8/n)\n","                Atom_in_Mg = Atom_Mg[Atom_Mg.iloc[:,1].between(min_x, max_x) & Atom_Mg.iloc[:,2].between(min_y, max_y) & Atom_Mg.iloc[:,3].between(min_z, max_z)]\n","                Atom_in_Al = Atom_Al[Atom_Al.iloc[:,1].between(min_x, max_x) & Atom_Al.iloc[:,2].between(min_y, max_y) & Atom_Al.iloc[:,3].between(min_z, max_z)]\n","                voxels[i,j,k, 0]=len(Atom_in_Mg)\n","                voxels[i,j,k, 1]=len(Atom_in_Al)\n","    return voxels\n","\n","all_data = np.empty([400,8,8,8,2])\n","\n","for i in tqdm(range(200)):\n","    folder_1 = 'simulated_data/data_fcc_{}.csv'.format(i)\n","    points = pd.read_csv(folder_1)\n","    voxels=voxelize(points)\n","    all_data[i] = voxels\n","    folder_2 = 'simulated_data/data_l12_{}.csv'.format(i)\n","    points = pd.read_csv(folder_2)\n","    voxels=voxelize(points)\n","    all_data[i+200] = voxels\n","    \n","all_y = np.empty([400,1])\n","\n","for i in tqdm(range(200)):\n","    all_y[i] = 0\n","    all_y[200+i] = 1\n","    \n","a = all_data[200,:,:,:,0]\n","b = all_data[200,:,:,:,1]\n","\n","    \n","bins     = [100,200,300]\n","bin_y =  pd.DataFrame(all_y[:,])\n","y_binned = np.digitize(bin_y.index, bins, right=True)\n"," \n","X_train, X_test, y_train, y_test = train_test_split(all_data, all_y, test_size=0.33, random_state=42, stratify=y_binned)\n","# with h5py.File(\"Fe_Al_data.h5\", \"a\") as f:\n","#     del f['y_train_16']\n","#     print(list(f.keys()))\n","hdf5_data = h5py.File('Fe_Al_data_8.h5','w')\n","hdf5_data.create_dataset('X_train_8',data= X_train)\n","hdf5_data.create_dataset('X_test_8',data= X_test)\n","hdf5_data.create_dataset('y_train_8',data= y_train)\n","hdf5_data.create_dataset('y_test_8',data= y_test)\n","hdf5_data.close()"]},{"cell_type":"markdown","metadata":{"id":"kDioCxj-7pBU"},"source":["#Classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EU33Urqh7tZA"},"outputs":[],"source":["# importing the libraries\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import os\n","\n","# for reading and displaying images\n","from skimage.io import imread\n","import matplotlib.pyplot as plt\n","\n","# for creating validation set\n","from sklearn.model_selection import train_test_split\n","# for evaluating the model\n","from sklearn.metrics import accuracy_score\n","import seaborn as sns\n","\n","# PyTorch libraries and modules\n","import torch\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import *\n","import h5py\n","# from plot3D import *\n","\n","with h5py.File(\"Fe_Al_data_8.h5\", \"r\") as hf:    \n","\n","    # Split the data into training/test features/targets\n","    X_train = hf[\"X_train_8\"][:]\n","    X_train = np.round(X_train)\n","    targets_train = hf[\"y_train_8\"][:]\n","    targets_train = np.round(targets_train)\n","    targets_train = np.int64(targets_train).flatten()\n","    \n","    X_test = hf[\"X_test_8\"][:] \n","    X_test = np.round(X_test)\n","    targets_test = hf[\"y_test_8\"][:]\n","    targets_test = np.round(targets_test)\n","    targets_test = np.int64(targets_test).flatten()\n","\n","    \n","train_x = torch.from_numpy(X_train).float()\n","train_y = torch.from_numpy(targets_train)\n","\n","\n","test_x = torch.from_numpy(X_test).float()\n","test_y = torch.from_numpy(targets_test)\n","\n","\n","batch_size = 4 #We pick beforehand a batch_size that we will use for the training\n","\n","\n","# Pytorch train and test sets\n","train = torch.utils.data.TensorDataset(train_x,train_y)\n","test = torch.utils.data.TensorDataset(test_x,test_y)\n","\n","# data loader\n","train_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\n","test_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = False)\n","\n","n_cubic=8\n","conv=2\n","# Create CNN Model\n","class CNNModel(nn.Module):\n","    def __init__(self):\n","        super(CNNModel, self).__init__()\n","        \n","        self.conv_layer1 = self._conv_layer_set(2, 32)\n","        self.conv_layer2 = self._conv_layer_set(32, 64)\n","        #self.conv_layer3 = self._conv_layer_set(64, 128)\n","        self.fc1 = nn.Linear(4**3*64, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.relu = nn.LeakyReLU()\n","        self.batch=nn.BatchNorm1d(128)\n","        self.drop=nn.Dropout(p=0.15)        \n","        \n","    def _conv_layer_set(self, in_c, out_c):\n","        conv_layer = nn.Sequential(\n","        nn.Conv3d(in_c, out_c, kernel_size=(3, 3, 3), padding=0),\n","        nn.LeakyReLU(),\n","        # nn.MaxPool3d((2, 2, 2)),\n","        )\n","        return conv_layer\n","    \n","\n","    def forward(self, x):\n","        # Set 1\n","        out = self.conv_layer1(x)\n","        out = self.conv_layer2(out)\n","        # out = self.conv_layer3(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.fc1(out)\n","        out = self.relu(out)\n","        out = self.batch(out)\n","        out = self.drop(out)\n","        out = self.fc2(out)\n","        \n","        return out\n","\n","#Definition of hyperparameters\n","n_iters = 4500\n","num_epochs = n_iters / (len(train_x) / batch_size)\n","num_epochs = int(num_epochs)\n","\n","# Create CNN\n","model = CNNModel()\n","#model.cuda()\n","print(model)\n","\n","# Cross Entropy Loss \n","# error = nn.CrossEntropyLoss()\n","error = nn.BCEWithLogitsLoss()\n","# error = nn.MSELoss()\n","\n","# SGD Optimizer\n","learning_rate = 0.01\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","# CNN model training\n","count = 0\n","loss_list = []\n","iteration_list = []\n","accuracy_list = []\n","train_ls, test_ls = [], []\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        \n","\n","        train = images.view(4,2,n_cubic,n_cubic,n_cubic)\n","        # labels = Variable(labels)\n","        \n","        # labels = torch.flatten(labels)\n","        \n","        # Clear gradients\n","        optimizer.zero_grad()\n","        # Forward propagation\n","        outputs = model(train)\n","        outputs = torch.flatten(outputs)\n","        #print(outputs)\n","        \n","        # Calculate softmax and ross entropy loss\n","        labels=labels.float()\n","        #print(labels)\n","        loss = error(outputs, labels)\n","        #print(loss)\n","        # Calculating gradients\n","        loss.backward()\n","        # Update parameters\n","        optimizer.step()\n","        \n","        count += 1\n","        if count % 50 == 0:\n","            # Calculate Accuracy         \n","            correct = 0\n","            total = 0\n","            # Iterate through test dataset\n","            for images, labels in test_loader:\n","                \n","                test = images.view(4,2,n_cubic,n_cubic,n_cubic)\n","                # Forward propagation\n","                outputs = model(test)\n","                print (outputs)\n","                outputs = torch.flatten(outputs)\n","                print (outputs)\n","                m=nn.Sigmoid()\n","                outputs = m(outputs)\n","                print (outputs)\n","\n","                # Get predictions from the maximum value\n","                for i in range(len(outputs)):\n","                   if outputs[i] >0.5:\n","                       outputs[i]=1\n","                   else:\n","                       outputs[i]=0\n","                print (outputs)\n","                #print (torch.max(outputs.data, 1))\n","                \n","                # Total number of labels\n","                total += len(labels)\n","                correct += (outputs == labels).sum()\n","                #print ('correct is {}'.format(correct))\n","                #print ('total is {}'.format(total))\n","                \n","            \n","            accuracy = 100 * correct / float(total)\n","            \n","            # store loss and iteration\n","            loss_list.append(loss.data)\n","            iteration_list.append(count)\n","            accuracy_list.append(accuracy)\n","       #if count % 50 == 0:\n","            #Print Loss\n","            #print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))\n","    train_x = train_x.view(268,2,n_cubic,n_cubic,n_cubic)\n","    test_x = test_x.view(132,2,n_cubic,n_cubic,n_cubic)\n","    train_y=train_y.float()\n","    test_y=test_y.float()\n","    train_ls.append(error(model(train_x).view(-1, 1), train_y.view(-1, 1)).item())\n","    test_ls.append(error(model(test_x).view(-1, 1), test_y.view(-1, 1)).item())\n","plt.close('all')\n","sns.set(style='darkgrid')\n","\n","print (\"plot curves\")\n","plt.figure()\n","plt.xlabel(\"epoch\")\n","plt.ylabel(\"loss\")\n","plt.plot(range(1, num_epochs+1),train_ls, linewidth = 5)\n","plt.plot(range(1, num_epochs+1),  test_ls, linewidth = 5, linestyle=':')\n","plt.legend(['train loss', 'test loss'])\n","plt.text(1500, 0.8, 'Loss=%.4f' % test_ls[-1], fontdict={'size': 20, 'color':  'red'})\n","# plt.show()\n","folder_dir = 'Figures'\n","if not os.path.isdir(folder_dir):\n","  os.mkdir(folder_dir)\n","plt.savefig('Figures/{}_{}.png'.format(n_cubic,conv), format='png', dpi=300)\n","\n","# plt.figure()\n","# plt.xlabel(\"epoch\")\n","# plt.ylabel(\"accuracy\")\n","# plt.plot(range(1, num_epochs+1),accuracy_list, linewidth = 5)\n","# plt.legend('accuracy')\n","# # plt.show()\n","# plt.savefig('Figures/accuracy_{}_{}.png'.format(n_cubic,conv), format='png', dpi=300)\n","\n","torch.save(model.state_dict(), 'Figures/{}_{}.pt'.format(n_cubic,conv))\n","d={'epoch': range(1, num_epochs+1), 'train_loss':train_ls, 'test_ls': test_ls}\n","data=pd.DataFrame(data=d)\n","accuracy_list=pd.DataFrame(accuracy_list)\n","train_ls=pd.DataFrame(train_ls)\n","test_ls=pd.DataFrame(test_ls)\n","train_ls.to_csv('Figures/{}_{}_train_loss.csv'.format(n_cubic,conv))\n","test_ls.to_csv('Figures/{}_{}_test_loss.csv'.format(n_cubic,conv))\n","accuracy_list.to_csv('Figures/{}_{}_accuracy_list.csv'.format(n_cubic,conv))\n","data.to_csv('Figures/{}_{}.csv'.format(n_cubic,conv))\n","print (\"=== train end ===\")"]},{"cell_type":"markdown","metadata":{"id":"noAT8cwCkoNV"},"source":["#Working on experimental data"]},{"cell_type":"markdown","metadata":{"id":"hI0pvo2Qkw_m"},"source":["##1. Cutting_cubes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61355,"status":"ok","timestamp":1681498957524,"user":{"displayName":"ziyuan rao","userId":"11142835866797234336"},"user_tz":-120},"id":"cvdyIkskk2uG","outputId":"01acb41c-988d-470c-aacf-2896b96eba0a"},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/10 [00:00<?, ?it/s]<ipython-input-59-a0e9efd7fa39>:64: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n","  cubic = s[s['z'].between((i), (i)+cube_size, inclusive=True)]\n","<ipython-input-59-a0e9efd7fa39>:66: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n","  p = cubic[cubic['y'].between((j), (j)+cube_size, inclusive=True)]\n","<ipython-input-59-a0e9efd7fa39>:68: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n","  x = p[p['x'].between((k),(k)+cube_size, inclusive=True)]\n","100%|██████████| 10/10 [01:01<00:00,  6.13s/it]\n"]}],"source":["import pandas as pd \n","import numpy as np \n","import os\n","from tqdm import tqdm\n","import re\n","import os\n","import time\n","\n","def read_rrng(f):\n","    rf = open(f,'r').readlines()\n","    patterns = re.compile(r'Ion([0-9]+)=([A-Za-z0-9]+).*|Range([0-9]+)=(\\d+.\\d+) +(\\d+.\\d+) +Vol:(\\d+.\\d+) +([A-Za-z:0-9 ]+) +Color:([A-Z0-9]{6})')\n","    ions = []\n","    rrngs = []\n","    for line in rf:\n","        m = patterns.search(line)\n","        if m:\n","            if m.groups()[0] is not None:\n","                ions.append(m.groups()[:2])\n","            else:\n","                rrngs.append(m.groups()[2:])\n","    ions = pd.DataFrame(ions, columns=['number','name'])\n","    ions.set_index('number',inplace=True)\n","    rrngs = pd.DataFrame(rrngs, columns=['number','lower','upper','vol','comp','colour'])\n","    rrngs.set_index('number',inplace=True) \n","    rrngs[['lower','upper','vol']] = rrngs[['lower','upper','vol']].astype(float)\n","    rrngs[['comp','colour']] = rrngs[['comp','colour']].astype(str)\n","    return ions,rrngs\n","\n","def atom_filter(x, Atom_range):\n","    Atom_total = pd.DataFrame()\n","    for i in range(len(Atom_range)):\n","        Atom = x[x['Da'].between(Atom_range['lower'][i], Atom_range['upper'][i], inclusive=True)]\n","        Atom_total = Atom_total.append(Atom)\n","        Count_Atom= len(Atom_total['Da'])   \n","    return Atom_total, Count_Atom  \n","\n","\n","rrange_file = 'AlLiMg.rrng' #this is the range file from APT\n","ions, rrngs = read_rrng(rrange_file)\n","Al_range = rrngs[rrngs['comp']=='Al:1']\n","Li_range = rrngs[rrngs['comp']=='Li:1']\n","Mg_range = rrngs[rrngs['comp']=='Mg:1']\n","\n","#%%all training cubics\n","folder = 'part_demo.csv' #the full APT data have more 100 M data points, we here show the demo part to save time. For full data pls contact the authors\n","cube_size = 4\n","step_size = 1\n","s= pd.read_csv(folder)\n","x_min = round(min(s['x']))\n","x_max = round(max(s['x']))\n","y_min = round(min(s['y']))\n","y_max = round(max(s['y']))\n","z_min = round(min(s['z']))\n","z_max = round(max(s['z']))   \n","p=[]\n","x=[]\n","folder_dir = 'cubic_4nm'\n","if not os.path.isdir(folder_dir):\n","    os.mkdir(folder_dir)\n","#cubic_total = pd.DataFrame()\n","\n","num_cubic=0\n","for i in tqdm(range(z_min,z_max,step_size)):\n","    cubic = s[s['z'].between((i), (i)+cube_size, inclusive=True)]\n","    for j in range(y_min,y_max,step_size):\n","        p = cubic[cubic['y'].between((j), (j)+cube_size, inclusive=True)]\n","        for k in range(x_min, x_max, step_size):\n","            x = p[p['x'].between((k),(k)+cube_size, inclusive=True)]\n","            if len(x['x'])>2000: #we only keep the cubic with atoms more than 2000, this should be adjusted according to the cubes size\n","                name ='{}/{}.csv'.format(folder_dir,num_cubic)\n","                num_cubic+=1\n","                x.to_csv(name, index=False)"]},{"cell_type":"code","source":["import shutil\n","folder_dir = 'matrix_4nm_CNN'\n","shutil.rmtree(folder_dir)\n","if os.path.isdir(folder_dir):\n","  print('ac') \n","if not os.path.isdir(folder_dir):\n","  print('abc')\n","  os.mkdir(folder_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f_UCL8y7CCMj","executionInfo":{"status":"ok","timestamp":1681504960888,"user_tz":-120,"elapsed":6625,"user":{"displayName":"ziyuan rao","userId":"11142835866797234336"}},"outputId":"2dbed0b6-6f76-445c-de5e-d755ed3c2d0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["abc\n"]}]},{"cell_type":"markdown","metadata":{"id":"qhH9BMnBtmLF"},"source":["##2. voxelization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4TJGTXVPttO1"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import re\n","from tqdm import tqdm\n","import h5py\n","from sklearn.model_selection import train_test_split\n","import os\n","\n","def read_rrng(f):\n","    rf = open(f,'r').readlines()\n","    patterns = re.compile(r'Ion([0-9]+)=([A-Za-z0-9]+).*|Range([0-9]+)=(\\d+.\\d+) +(\\d+.\\d+) +Vol:(\\d+.\\d+) +([A-Za-z:0-9 ]+) +Color:([A-Z0-9]{6})')\n","    ions = []\n","    rrngs = []\n","    for line in rf:\n","        m = patterns.search(line)\n","        if m:\n","            if m.groups()[0] is not None:\n","                ions.append(m.groups()[:2])\n","            else:\n","                rrngs.append(m.groups()[2:])\n","    ions = pd.DataFrame(ions, columns=['number','name'])\n","    ions.set_index('number',inplace=True)\n","    rrngs = pd.DataFrame(rrngs, columns=['number','lower','upper','vol','comp','colour'])\n","    rrngs.set_index('number',inplace=True) \n","    rrngs[['lower','upper','vol']] = rrngs[['lower','upper','vol']].astype(float)\n","    rrngs[['comp','colour']] = rrngs[['comp','colour']].astype(str)\n","    return ions,rrngs\n","\n","def atom_filter(x, Atom_range):\n","    Atom_total = pd.DataFrame()\n","    for i in range(len(Atom_range)):\n","        Atom = x[x['Da'].between(Atom_range['lower'][i], Atom_range['upper'][i], inclusive=True)]\n","        Atom_total = Atom_total.append(Atom)\n","        Count_Atom= len(Atom_total['Da'])   \n","    return Atom_total, Count_Atom  \n","#%%read range file\n","rrange_file = 'AlLiMg.rrng'\n","ions, rrngs = read_rrng(rrange_file)\n","Al_range = rrngs[rrngs['comp']=='Al:1']\n","Li_range = rrngs[rrngs['comp']=='Li:1']\n","Mg_range = rrngs[rrngs['comp']=='Mg:1']\n","\n","\n","def voxelize(points):\n","    \"\"\"\n","    Convert `points` to centerlized voxel with size `voxel_size` and `resolution`, then padding zero to\n","    `padding_to_size`. The outside part is cut, rather than scaling the points.\n","    Args:\n","    `points`: pointcloud in 3D numpy.ndarray\n","    `voxel_size`: the centerlized voxel size, default (24,24,24)\n","    `padding_to_size`: the size after zero-padding, default (32,32,32)\n","    `resolution`: the resolution of voxel, in meters\n","    Ret:\n","    `voxel`:32*32*32 voxel occupany grid\n","    `inside_box_points`:pointcloud inside voxel grid\n","    \"\"\"\n","    n = 8\n","    padding_size=(n, n, n, 2)\n","    voxels = np.zeros(padding_size)\n","    points = points.values\n","    origin = (np.min(points[:, 0]), np.min(points[:, 1]), np.min(points[:, 2]))\n","    points[:, 0] -= origin[0]\n","    points[:, 1] -= origin[1]\n","    points[:, 2] -= origin[2]\n","    \n","    points=pd.DataFrame(points,columns=['x','y','z','Da'])\n","    \n","    Atom_Li, Count_Li = atom_filter(points, Li_range)    \n","    Atom_Mg, Count_Mg = atom_filter(points, Mg_range)\n","    Atom_Al, Count_Al = atom_filter(points, Al_range)\n","    #print(Count_Li+Count_Mg+Count_Al)\n","    \n","    for i in range(n):\n","        for j in range(n):\n","            for k in range(n):\n","                min_x = i*(4/n)\n","                max_x = (i+1)*(4/n)\n","                min_y = j*(4/n)\n","                max_y = (j+1)*(4/n)\n","                min_z = k*(4/n)\n","                max_z = (k+1)*(4/n)\n","                Atom_in_Li = Atom_Li[Atom_Li.iloc[:,0].between(min_x, max_x) & Atom_Li.iloc[:,1].between(min_y, max_y) & Atom_Li.iloc[:,2].between(min_z, max_z)]\n","                Atom_in_Mg = Atom_Mg[Atom_Mg.iloc[:,0].between(min_x, max_x) & Atom_Mg.iloc[:,1].between(min_y, max_y) & Atom_Mg.iloc[:,2].between(min_z, max_z)]\n","                Atom_in_Al = Atom_Al[Atom_Al.iloc[:,0].between(min_x, max_x) & Atom_Al.iloc[:,1].between(min_y, max_y) & Atom_Al.iloc[:,2].between(min_z, max_z)]\n","                voxels[i,j,k, 0]=len(Atom_in_Mg)+len(Atom_in_Li)\n","                voxels[i,j,k, 1]=len(Atom_in_Al)\n","    #print(voxels.sum())\n","                \n","                \n","    return voxels\n","all_data = np.empty([0,8,8,8,2])\n","folder='cubic_4nm'\n","for filename in tqdm(os.listdir(folder)):\n","    points = pd.read_csv(folder+'/'+filename)\n","    voxels=voxelize(points)\n","    voxels=np.expand_dims(voxels,axis=0)\n","    print(voxels.sum())\n","    all_data = np.append(all_data,voxels,axis=0)\n","    \n","\n","hdf5_data = h5py.File('cubic_4nm.h5','w')\n","hdf5_data.create_dataset('all_data',data= all_data)\n","hdf5_data.close()"]},{"cell_type":"markdown","metadata":{"id":"_Y03yEKmn5U1"},"source":["##3. Preidict with experimental data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KXXieAQDn_oj"},"outputs":[],"source":["# importing the libraries\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import os\n","from glob import glob\n","\n","# for reading and displaying images\n","from skimage.io import imread\n","import matplotlib.pyplot as plt\n","\n","# for creating validation set\n","from sklearn.model_selection import train_test_split\n","# for evaluating the model\n","from sklearn.metrics import accuracy_score\n","import seaborn as sns\n","\n","# PyTorch libraries and modules\n","import torch\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import *\n","import h5py\n","# from plot3D import *\n","\n","n_cubic=8\n","conv=2\n","# Create CNN Model\n","class CNNModel(nn.Module):\n","    def __init__(self):\n","        super(CNNModel, self).__init__()\n","        \n","        self.conv_layer1 = self._conv_layer_set(2, 32)\n","        self.conv_layer2 = self._conv_layer_set(32, 64)\n","        # self.conv_layer3 = self._conv_layer_set(64, 128)\n","        self.fc1 = nn.Linear(4**3*64, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.relu = nn.LeakyReLU()\n","        self.batch=nn.BatchNorm1d(128)\n","        self.drop=nn.Dropout(p=0.15)        \n","        \n","    def _conv_layer_set(self, in_c, out_c):\n","        conv_layer = nn.Sequential(\n","        nn.Conv3d(in_c, out_c, kernel_size=(3, 3, 3), padding=0),\n","        # nn.LeakyReLU(),\n","        # nn.MaxPool3d((2, 2, 2)),\n","        )\n","        return conv_layer\n","    \n","\n","    def forward(self, x):\n","        # Set 1\n","        out = self.conv_layer1(x)\n","\n","        out = self.conv_layer2(out)\n","\n","        # out = self.conv_layer3(out)\n","        out = out.view(out.size(0), -1)\n","\n","        out = self.fc1(out)\n","\n","        # out = self.relu(out)\n","        out = self.batch(out)\n","        # out = self.drop(out)\n","        out = self.fc2(out)\n","        \n","        return out\n","\n","#Definition of hyperparameters\n","#n_iters = 8000\n","#num_epochs = n_iters / (len(train_x) / batch_size)\n","# num_epochs = int(num_epochs)\n","#num_epochs=1\n","\n","# Create CNN\n","model = CNNModel()\n","#model.cuda()\n","print(model)\n","\n","# Cross Entropy Loss \n","# error = nn.CrossEntropyLoss()\n","error = nn.BCEWithLogitsLoss()\n","# error = nn.MSELoss()\n","\n","# SGD Optimizer\n","learning_rate = 0.01\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","#%%load model\n","model = CNNModel()\n","model.load_state_dict(torch.load('{}_{}.pt'.format(n_cubic,conv)))\n","model.eval()\n","\n","with h5py.File(\"cubic_4nm.h5\", \"r\") as hf: \n","    print(hf.keys())\n","\n","    \n","    X_test = hf[\"all_data\"][:] \n","    X_test = np.round(X_test)\n","\n","test = torch.from_numpy(X_test).float()\n","test = test.view(len(test),2,n_cubic,n_cubic,n_cubic)\n","# test = test.view(len(new_X_test),2,n_cubic,n_cubic,n_cubic)\n","results = model(test)\n","print (results)\n","results = torch.flatten(results)\n","r=results.data.numpy()\n","print (results)\n","m=nn.Sigmoid()\n","results = m(results)\n","print (results)\n","for i in range(len(results)):\n","   if results[i] >0.5:\n","       results[i]=1\n","   else:\n","       results[i]=0\n","print (results)\n","\n","Y_test = results.data.numpy()\n","\n","    \n","folder_1='precipitation_4nm_CNN'\n","folder_2='matrix_4nm_CNN'\n","if not os.path.isdir(folder_1):\n","    os.mkdir(folder_1)\n","if not os.path.isdir(folder_2):\n","    os.mkdir(folder_2)\n","\n","    \n","# cubic = glob(\"precipitation_4nm/*\")\n","folder = 'cubic_4nm'\n","# for i in tqdm(range(len(cubic))):\n","listdir_all = os.listdir(folder)\n","i=0\n","for filename in tqdm(listdir_all):\n","    if Y_test[i]==0:\n","        matrix = pd.read_csv(folder+'/'+filename)\n","        matrix.to_csv('matrix_4nm_CNN'+'/'+filename)\n","\n","    if Y_test[i]==1:\n","        precipitation = pd.read_csv(folder+'/'+filename)\n","        precipitation.to_csv('precipitation_4nm_CNN'+'/'+filename)\n","    i+=1\n","        \n","folder='precipitation_4nm_CNN'\n","pre = pd.DataFrame()\n","for filename in tqdm(os.listdir(folder)):\n","    x = pd.read_csv(folder+'/'+filename)\n","    pre=pre.append(x)\n","    \n","pre_1=pre.groupby(['x','y','z','Da'], as_index=False)['x'].agg({'cnt':'count'})\n","pre_1.to_csv('precipitation_4nm_CNN_with_score.csv',index=False)\n","\n","folder='matrix_4nm_CNN'\n","pre = pd.DataFrame()\n","for filename in tqdm(os.listdir(folder)):\n","    x = pd.read_csv(folder+'/'+filename)\n","    pre=pre.append(x)\n","    \n","pre_2=pre.groupby(['x','y','z','Da'], as_index=False)['x'].agg({'cnt':'count'})\n","pre_2['cnt']=pre_2['cnt']*(-1)\n","\n","pre_2.to_csv('matrix_4nm_CNN_with_score.csv',index=False)\n","\n","precipitation = pd.read_csv('precipitation_4nm_CNN_with_score.csv')\n","matrix = pd.read_csv('matrix_4nm_CNN_with_score.csv')\n","\n"]},{"cell_type":"markdown","source":["##4. Get the final precipitation"],"metadata":{"id":"MzGOSA8R2XCD"}},{"cell_type":"code","source":["import pandas as pd\n","precipitation = pd.read_csv('precipitation_4nm_CNN_with_score.csv')\n","matrix = pd.read_csv('matrix_4nm_CNN_with_score.csv')\n","\n","all_atoms=pd.DataFrame()\n","all_atoms=all_atoms.append(precipitation)\n","all_atoms=all_atoms.append(matrix)\n","\n","res=all_atoms.groupby(['x','y','z','Da'], as_index=False).agg({'cnt':'sum'})\n","pre=res[res['cnt']>0]\n","\n","\n","precipitation = precipitation[['x','y','z','Da']]\n","precipitation.to_csv('precipitation_4nm_CNN_final.csv',index=False)"],"metadata":{"id":"4VaSbnsX2dn0"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["--iZtJK0CtKs","PS6_vOnr-0w_","lBdO5Y6A_HL-","PLpFywjzGiSK","kDioCxj-7pBU","hI0pvo2Qkw_m","qhH9BMnBtmLF","_Y03yEKmn5U1","MzGOSA8R2XCD"],"provenance":[],"authorship_tag":"ABX9TyM4P/JAt2mgJHk/Ub2N1Cyp"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}